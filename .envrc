dotenv

if [ -n "$IN_NIX_SHELL" ]; then
    # Already in nix shell, skip everything
    return
fi

export DIRENV_WARN_TIMEOUT="${DIRENV_WARN_TIMEOUT:-300s}"

FLAKE_HASH=$(nix flake metadata . --json 2>/dev/null | jq -r .lastModified || echo "unknown")
CACHED_PROFILE="$HOME/.cache/direnv/nix-profiles/lana-$FLAKE_HASH"

if [ -d "$CACHED_PROFILE" ]; then
    # Use cached profile
    export PATH="$CACHED_PROFILE/bin:$PATH"
    export IN_NIX_SHELL=1
else
    # Build profile in background
    echo "Building Nix environment (this may take a while first time)..."
    if [ -n "$REMOTE_CONTAINERS" ]; then
        use flake . --impure
    else
        # Show progress
        nix develop . --profile "$CACHED_PROFILE" --command true && \
        use flake .
    fi
fi

# Rest of your config (only runs if not skipped above)
# Cache the expensive operations
if [ -z "$_ENVRC_CREDS_CACHED" ]; then
    export _ENVRC_CREDS_CACHED=1

    # Move keyfile to container-local path in devcontainer
    if [ -n "$REMOTE_CONTAINERS" ]; then
        KEYFILE_PATH="/tmp/keyfile.json"
    else
        KEYFILE_PATH="$(pwd)/meltano/keyfile.json"
    fi

    # Only decode once if TF_VAR_sa_creds exists
    if [ -n "$TF_VAR_sa_creds" ]; then
        export _DECODED_CREDS="$(echo $TF_VAR_sa_creds | base64 -d)"
        export _PROJECT_ID="$(echo $_DECODED_CREDS | jq -r '.project_id')"

        mkdir -p "$(dirname $KEYFILE_PATH)"
        echo $_DECODED_CREDS > $KEYFILE_PATH
        export GOOGLE_APPLICATION_CREDENTIALS="$KEYFILE_PATH"
        export DBT_BIGQUERY_KEYFILE="$KEYFILE_PATH"
    fi
fi

# Only set these if the cached values exist
[ -n "$_DECODED_CREDS" ] && export TARGET_BIGQUERY_CREDENTIALS_JSON="$_DECODED_CREDS"
[ -n "$_PROJECT_ID" ] && export DBT_BIGQUERY_PROJECT="$_PROJECT_ID"

# Rest of exports
export TF_VAR_name_prefix="${USER}"
export SA_CREDS_BASE64="${TF_VAR_sa_creds}"
export DEV_ENV_NAME_PREFIX="${TF_VAR_name_prefix}"
export DATAFORM_SCHEMA_SUFFIX=${TF_VAR_name_prefix}
export TARGET_BIGQUERY_DATASET="${USER}_dataset"
export TARGET_BIGQUERY_LOCATION="US"
export DBT_BIGQUERY_DATASET="dbt_${USER}"
export DOCS_BUCKET_NAME="${USER}-lana-documents"
export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://user:password@localhost:5436/pg"
export AIRFLOW__CORE__EXECUTOR=LocalExecutor
